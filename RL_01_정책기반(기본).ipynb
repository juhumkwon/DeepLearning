{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiQTlAPEGqXIhAHfOqEBxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/DeepLearning/blob/main/RL_01_%EC%A0%95%EC%B1%85%EA%B8%B0%EB%B0%98(%EA%B8%B0%EB%B3%B8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V79SbGLxGrLf",
        "outputId": "56dc0415-f0b7-44b6-82dc-371deb19fd14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "상태: [ 0.02296786 -0.01010182 -0.04752216 -0.005504  ]\n",
            "행동 확률분포: [0.49969834 0.50030166]\n",
            "선택된 행동: 1\n",
            "보상: 1.0, 다음 상태: [ 0.02276583  0.18566832 -0.04763224 -0.31279394], 종료 여부: False\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gymnasium as gym # Changed from gym to gymnasium\n",
        "\n",
        "# 1. 정책 신경망 정의\n",
        "class PolicyNet(tf.keras.Model):\n",
        "    def __init__(self, action_size):\n",
        "        super().__init__()\n",
        "        self.d1 = tf.keras.layers.Dense(24, activation='relu')\n",
        "        self.out = tf.keras.layers.Dense(action_size)  # 로짓 출력\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.d1(x)\n",
        "        logits = self.out(x)\n",
        "        action_probs = tf.nn.softmax(logits)  # 확률로 변환\n",
        "        return action_probs\n",
        "\n",
        "# 2. 환경 & 네트워크 초기화\n",
        "# env = gym.make(\"CartPole-v1\") # Original line\n",
        "env = gym.make(\"CartPole-v1\") # Removed new_step_api=True\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "policy = PolicyNet(action_size)\n",
        "\n",
        "# 3. 상태 입력 → 확률 계산 → 행동 샘플링 → 환경 적용\n",
        "# state = env.reset()[0] # Original line\n",
        "state, info = env.reset() # Updated to handle tuple return from gymnasium reset\n",
        "state_tensor = tf.convert_to_tensor(state[None, :], dtype=tf.float32)\n",
        "\n",
        "action_probs = policy(state_tensor).numpy()[0]  # 확률 분포\n",
        "action = np.random.choice(action_size, p=action_probs)  # 확률적으로 행동 선택\n",
        "\n",
        "# next_state, reward, done, _, _ = env.step(action) # Original line\n",
        "next_state, reward, terminated, truncated, info = env.step(action) # Updated to handle gymnasium step return\n",
        "done = terminated or truncated # Combine terminated and truncated for 'done'\n",
        "\n",
        "# 4. 결과 출력\n",
        "print(f\"상태: {state}\")\n",
        "print(f\"행동 확률분포: {action_probs}\")\n",
        "print(f\"선택된 행동: {action}\")\n",
        "print(f\"보상: {reward}, 다음 상태: {next_state}, 종료 여부: {done}\")"
      ]
    }
  ]
}