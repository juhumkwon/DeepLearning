{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMb2/1GfbQluC/FLyXaJiuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/DeepLearning/blob/main/RL_03_3_1_ppo_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXNn5dx-v-jN",
        "outputId": "27245199-c921-4c20-97fe-f2151445a20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Reward=23.0, Loss=30.524, PolicyLoss=-8.068, ValueLoss=77.343, Entropy=7.961\n",
            "Episode 1: Reward=25.0, Loss=33.773, PolicyLoss=-8.454, ValueLoss=84.628, Entropy=8.686\n",
            "Episode 2: Reward=12.0, Loss=11.502, PolicyLoss=-5.259, ValueLoss=33.605, Entropy=4.159\n",
            "Episode 3: Reward=30.0, Loss=41.512, PolicyLoss=-9.326, ValueLoss=101.885, Entropy=10.401\n",
            "Episode 4: Reward=21.0, Loss=27.195, PolicyLoss=-7.551, ValueLoss=69.629, Entropy=6.845\n",
            "Episode 5: Reward=11.0, Loss=9.810, PolicyLoss=-4.924, ValueLoss=29.543, Entropy=3.814\n",
            "Episode 6: Reward=16.0, Loss=18.315, PolicyLoss=-6.406, ValueLoss=49.553, Entropy=5.530\n",
            "Episode 7: Reward=10.0, Loss=8.013, PolicyLoss=-4.430, ValueLoss=24.948, Entropy=3.142\n",
            "Episode 8: Reward=14.0, Loss=14.839, PolicyLoss=-5.836, ValueLoss=41.448, Entropy=4.853\n",
            "Episode 9: Reward=35.0, Loss=48.772, PolicyLoss=-10.072, ValueLoss=117.931, Entropy=12.149\n",
            "Episode 10: Reward=17.0, Loss=20.018, PolicyLoss=-6.652, ValueLoss=53.457, Entropy=5.911\n",
            "Episode 11: Reward=19.0, Loss=23.452, PolicyLoss=-7.082, ValueLoss=61.194, Entropy=6.362\n",
            "Episode 12: Reward=27.0, Loss=37.271, PolicyLoss=-8.788, ValueLoss=92.299, Entropy=8.982\n",
            "Episode 13: Reward=21.0, Loss=26.953, PolicyLoss=-7.609, ValueLoss=69.269, Entropy=7.281\n",
            "Episode 14: Reward=26.0, Loss=35.328, PolicyLoss=-8.626, ValueLoss=88.088, Entropy=9.031\n",
            "Episode 15: Reward=21.0, Loss=26.945, PolicyLoss=-7.560, ValueLoss=69.151, Entropy=7.046\n",
            "Episode 16: Reward=17.0, Loss=19.767, PolicyLoss=-6.545, ValueLoss=52.737, Entropy=5.653\n",
            "Episode 17: Reward=15.0, Loss=16.516, PolicyLoss=-6.060, ValueLoss=45.253, Entropy=5.081\n",
            "Episode 18: Reward=21.0, Loss=26.978, PolicyLoss=-7.601, ValueLoss=69.303, Entropy=7.287\n",
            "Episode 19: Reward=24.0, Loss=31.748, PolicyLoss=-8.197, ValueLoss=80.055, Entropy=8.310\n",
            "Episode 20: Reward=11.0, Loss=9.539, PolicyLoss=-4.739, ValueLoss=28.627, Entropy=3.557\n",
            "Episode 21: Reward=43.0, Loss=58.430, PolicyLoss=-10.997, ValueLoss=139.148, Entropy=14.785\n",
            "Episode 22: Reward=20.0, Loss=25.292, PolicyLoss=-7.314, ValueLoss=65.347, Entropy=6.744\n",
            "Episode 23: Reward=13.0, Loss=12.611, PolicyLoss=-5.418, ValueLoss=36.148, Entropy=4.507\n",
            "Episode 24: Reward=13.0, Loss=13.043, PolicyLoss=-5.520, ValueLoss=37.217, Entropy=4.504\n",
            "Episode 25: Reward=23.0, Loss=30.348, PolicyLoss=-8.030, ValueLoss=76.915, Entropy=7.967\n",
            "Episode 26: Reward=20.0, Loss=25.034, PolicyLoss=-7.296, ValueLoss=64.797, Entropy=6.863\n",
            "Episode 27: Reward=21.0, Loss=26.572, PolicyLoss=-7.560, ValueLoss=68.409, Entropy=7.284\n",
            "Episode 28: Reward=19.0, Loss=23.355, PolicyLoss=-7.101, ValueLoss=61.044, Entropy=6.610\n",
            "Episode 29: Reward=9.0, Loss=6.388, PolicyLoss=-4.011, ValueLoss=20.857, Entropy=2.890\n",
            "Episode 30: Reward=15.0, Loss=16.438, PolicyLoss=-6.071, ValueLoss=45.123, Entropy=5.209\n",
            "Episode 31: Reward=13.0, Loss=12.465, PolicyLoss=-5.337, ValueLoss=35.691, Entropy=4.364\n",
            "Episode 32: Reward=19.0, Loss=23.249, PolicyLoss=-7.038, ValueLoss=60.705, Entropy=6.538\n",
            "Episode 33: Reward=11.0, Loss=9.306, PolicyLoss=-4.684, ValueLoss=28.055, Entropy=3.666\n",
            "Episode 34: Reward=12.0, Loss=11.167, PolicyLoss=-5.083, ValueLoss=32.582, Entropy=4.080\n",
            "Episode 35: Reward=14.0, Loss=14.586, PolicyLoss=-5.768, ValueLoss=40.805, Entropy=4.868\n",
            "Episode 36: Reward=27.0, Loss=36.818, PolicyLoss=-8.730, ValueLoss=91.281, Entropy=9.283\n",
            "Episode 37: Reward=25.0, Loss=33.583, PolicyLoss=-8.350, ValueLoss=84.036, Entropy=8.545\n",
            "Episode 38: Reward=9.0, Loss=6.319, PolicyLoss=-3.967, ValueLoss=20.632, Entropy=2.984\n",
            "Episode 39: Reward=35.0, Loss=48.400, PolicyLoss=-10.013, ValueLoss=117.068, Entropy=12.053\n",
            "Episode 40: Reward=32.0, Loss=44.159, PolicyLoss=-9.589, ValueLoss=107.718, Entropy=11.107\n",
            "Episode 41: Reward=14.0, Loss=14.415, PolicyLoss=-5.657, ValueLoss=40.237, Entropy=4.684\n",
            "Episode 42: Reward=24.0, Loss=31.999, PolicyLoss=-8.173, ValueLoss=80.508, Entropy=8.248\n",
            "Episode 43: Reward=33.0, Loss=45.882, PolicyLoss=-9.707, ValueLoss=111.402, Entropy=11.227\n",
            "Episode 44: Reward=15.0, Loss=16.292, PolicyLoss=-6.028, ValueLoss=44.745, Entropy=5.221\n",
            "Episode 45: Reward=22.0, Loss=28.006, PolicyLoss=-7.746, ValueLoss=71.658, Entropy=7.673\n",
            "Episode 46: Reward=11.0, Loss=9.525, PolicyLoss=-4.680, ValueLoss=28.484, Entropy=3.688\n",
            "Episode 47: Reward=27.0, Loss=36.574, PolicyLoss=-8.762, ValueLoss=90.858, Entropy=9.378\n",
            "Episode 48: Reward=23.0, Loss=29.786, PolicyLoss=-7.953, ValueLoss=75.638, Entropy=7.981\n",
            "Episode 49: Reward=20.0, Loss=24.921, PolicyLoss=-7.204, ValueLoss=64.382, Entropy=6.645\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ----- 1. Actor-Critic 모델 정의 -----\n",
        "class ActorCritic(tf.keras.Model):\n",
        "    def __init__(self, action_dim):\n",
        "        super().__init__()\n",
        "        self.common = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.actor = tf.keras.layers.Dense(action_dim, activation='softmax')\n",
        "        self.critic = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.common(x)\n",
        "        return self.actor(x), self.critic(x)\n",
        "\n",
        "# ----- 2. Advantage 계산 함수 -----\n",
        "def compute_advantages(rewards, values, dones, gamma=0.99, lam=0.95):\n",
        "    advantages, gae = [], 0\n",
        "    next_value = 0\n",
        "    for r, v, d in zip(reversed(rewards), reversed(values), reversed(dones)):\n",
        "        delta = r + gamma * next_value * (1 - d) - v\n",
        "        gae = delta + gamma * lam * (1 - d) * gae\n",
        "        advantages.insert(0, gae)\n",
        "        next_value = v\n",
        "    returns = np.array(advantages) + np.array(values)\n",
        "    return np.array(advantages, dtype=np.float32), returns.astype(np.float32)\n",
        "\n",
        "# ----- 3. PPO Loss -----\n",
        "def ppo_loss(old_log_probs, new_log_probs, advantages, values, returns, clip_ratio=0.2,\n",
        "             c_v=0.5, c_e=0.01):\n",
        "    # ratio = exp(new - old)\n",
        "    ratio = tf.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "    # Surrogate objectives\n",
        "    surr1 = ratio * advantages\n",
        "    surr2 = tf.clip_by_value(ratio, 1.0 - clip_ratio, 1.0 + clip_ratio) * advantages\n",
        "    policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))\n",
        "\n",
        "    # Value loss (MSE)\n",
        "    value_loss = tf.reduce_mean(tf.square(returns - values))\n",
        "\n",
        "    # Entropy (탐험 보너스)\n",
        "    entropy = -tf.reduce_mean(tf.reduce_sum(tf.exp(new_log_probs) * new_log_probs, axis=-1))\n",
        "\n",
        "    total_loss = policy_loss + c_v * value_loss - c_e * entropy\n",
        "    return total_loss, policy_loss, value_loss, entropy\n",
        "\n",
        "# ----- 4. 학습 루프 -----\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "model = ActorCritic(action_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
        "\n",
        "for episode in range(50):\n",
        "    obs, _ = env.reset()\n",
        "    done, ep_reward = False, 0\n",
        "\n",
        "    observations, actions, rewards, dones, values, log_probs = [], [], [], [], [], []\n",
        "\n",
        "    while not done:\n",
        "        obs_tensor = tf.convert_to_tensor(obs[None, :], dtype=tf.float32)\n",
        "        probs, value = model(obs_tensor)\n",
        "        action = np.random.choice(action_dim, p=probs.numpy()[0])\n",
        "\n",
        "        # Log prob for PPO ratio\n",
        "        log_prob = tf.math.log(probs[0, action] + 1e-8)\n",
        "\n",
        "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Save trajectory\n",
        "        observations.append(obs)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        dones.append(done)\n",
        "        values.append(value.numpy()[0,0])\n",
        "        log_probs.append(log_prob.numpy())\n",
        "\n",
        "        obs = next_obs\n",
        "        ep_reward += reward\n",
        "\n",
        "    # ----- Advantage & Return 계산 -----\n",
        "    advantages, returns = compute_advantages(rewards, values, dones)\n",
        "\n",
        "    # ----- PPO 업데이트 -----\n",
        "    with tf.GradientTape() as tape:\n",
        "        obs_tensor = tf.convert_to_tensor(np.array(observations), dtype=tf.float32)\n",
        "        probs, values_pred = model(obs_tensor)\n",
        "\n",
        "        # 새 log probs\n",
        "        indices = tf.range(len(actions))\n",
        "        action_indices = tf.stack([indices, actions], axis=1)\n",
        "        new_probs = tf.gather_nd(probs, action_indices)\n",
        "        new_log_probs = tf.math.log(new_probs + 1e-8)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss, pl, vl, ent = ppo_loss(\n",
        "            old_log_probs=tf.convert_to_tensor(log_probs, dtype=tf.float32),\n",
        "            new_log_probs=new_log_probs,\n",
        "            advantages=tf.convert_to_tensor(advantages, dtype=tf.float32),\n",
        "            values=tf.squeeze(values_pred),\n",
        "            returns=tf.convert_to_tensor(returns, dtype=tf.float32),\n",
        "            clip_ratio=0.2,\n",
        "            c_v=0.5,\n",
        "            c_e=0.01\n",
        "        )\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    print(f\"Episode {episode}: Reward={ep_reward:.1f}, \"\n",
        "          f\"Loss={loss.numpy():.3f}, PolicyLoss={pl.numpy():.3f}, \"\n",
        "          f\"ValueLoss={vl.numpy():.3f}, Entropy={ent.numpy():.3f}\")"
      ]
    }
  ]
}