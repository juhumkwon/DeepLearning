{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "RL_02_03_reinforce_basic.ipynb",
      "authorship_tag": "ABX9TyNvgyHlpH8k1Appg5JZ+WOn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/DeepLearning/blob/main/RL_02_03_reinforce_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5d97Io6iZxz",
        "outputId": "6a227f64-fef5-4265-feb6-62e520548335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: total reward = 22.0\n",
            "Episode 2: total reward = 15.0\n",
            "Episode 3: total reward = 96.0\n"
          ]
        }
      ],
      "source": [
        "# import gym # Deprecated\n",
        "import gymnasium as gym # Use gymnasium instead of gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ----------------------------\n",
        "# 환경 설정\n",
        "# ----------------------------\n",
        "# env = gym.make(\"CartPole-v1\") # Old API\n",
        "env = gym.make(\"CartPole-v1\") # Removed new_step_api=True\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "# ----------------------------\n",
        "# 정책 신경망 정의\n",
        "# ----------------------------\n",
        "class PolicyNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d1 = tf.keras.layers.Dense(24, activation='relu')\n",
        "        self.d2 = tf.keras.layers.Dense(24, activation='relu')\n",
        "        self.logits = tf.keras.layers.Dense(action_dim)  # 로짓 출력\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        return self.logits(x)\n",
        "\n",
        "policy = PolicyNet()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# ----------------------------\n",
        "# 액션 선택 함수\n",
        "# ----------------------------\n",
        "def select_action(state):\n",
        "    state = tf.expand_dims(state, 0)  # 배치 차원 추가\n",
        "    logits = policy(state)\n",
        "    action_prob = tf.nn.softmax(logits)\n",
        "    action = tf.random.categorical(tf.math.log(action_prob), 1)[0,0].numpy()\n",
        "    return action, action_prob[0]\n",
        "\n",
        "# ----------------------------\n",
        "# 에피소드 실행 및 학습\n",
        "# ----------------------------\n",
        "gamma = 0.99  # 감가율\n",
        "\n",
        "for episode in range(3):  # 짧은 예시용\n",
        "    state, info = env.reset() # Gymnasium reset returns state and info\n",
        "    done = False\n",
        "    states, actions, rewards = [], [], []\n",
        "\n",
        "    while not done:\n",
        "        action, action_prob = select_action(state)\n",
        "        # next_state, reward, done, _ = env.step(action) # Old API\n",
        "        next_state, reward, terminated, truncated, info = env.step(action) # Gymnasium step returns 5 values\n",
        "        done = terminated or truncated # done is now the logical OR of terminated and truncated\n",
        "\n",
        "        # 데이터 기록\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # ----------------------------\n",
        "    # 에피소드 종료 후 REINFORCE 업데이트\n",
        "    # ----------------------------\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.insert(0, G)\n",
        "    returns = np.array(returns, dtype=np.float32)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # 정규화\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = 0\n",
        "        for s, a, Gt in zip(states, actions, returns):\n",
        "            s = tf.expand_dims(s, 0)\n",
        "            logits = policy(s)\n",
        "            pi = tf.nn.softmax(logits)[0]\n",
        "            # 로짓 기준 로그 확률 그라디언트\n",
        "            log_prob = tf.math.log(tf.reduce_sum(pi * tf.one_hot(a, action_dim)))\n",
        "            loss += -log_prob * Gt  # gradient ascent -> minimize -log_prob*Gt\n",
        "\n",
        "    grads = tape.gradient(loss, policy.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, policy.trainable_variables))\n",
        "\n",
        "    print(f\"Episode {episode+1}: total reward = {sum(rewards)}\")"
      ]
    }
  ]
}