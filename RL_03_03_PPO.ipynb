{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfJp/vTRthx5FAWIjzlHG6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/DeepLearning/blob/main/RL_03_03_PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElYQdIfg2q5y",
        "outputId": "0200ac99-754e-432f-a1c4-5628b888dcaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: gymnasium\n",
            "Start training PPO on CartPole-v1\n",
            "Step    2048 | loss_pi: -0.0175  loss_v: 6.7264  entropy: 0.6587  ratio: 1.007  adv μ/σ: 10.196/4.549\n",
            "Step    4096 | loss_pi: -0.0408  loss_v: 21.2399  entropy: 0.6525  ratio: 0.981  adv μ/σ: 7.045/6.874\n",
            "Step    6144 | loss_pi: -0.0014  loss_v: 17.7267  entropy: 0.6026  ratio: 0.998  adv μ/σ: 6.855/7.602\n",
            "Step    8192 | loss_pi: -0.0246  loss_v: 28.3640  entropy: 0.6322  ratio: 0.968  adv μ/σ: 7.766/7.083\n",
            "[Step 10000] Eval: 196.4 ± 13.6\n",
            "Step   10240 | loss_pi: 0.0176  loss_v: 26.7332  entropy: 0.5668  ratio: 1.050  adv μ/σ: 4.616/9.070\n",
            "Step   12288 | loss_pi: -0.0036  loss_v: 48.9482  entropy: 0.5636  ratio: 0.972  adv μ/σ: 3.252/9.517\n",
            "Step   14336 | loss_pi: 0.0433  loss_v: 1.9091  entropy: 0.6174  ratio: 0.936  adv μ/σ: 4.645/5.593\n",
            "Step   16384 | loss_pi: 0.1060  loss_v: 91.7638  entropy: 0.4015  ratio: 1.181  adv μ/σ: -11.737/17.225\n",
            "Step   18432 | loss_pi: 0.0509  loss_v: 1.2866  entropy: 0.6402  ratio: 1.014  adv μ/σ: 5.958/4.519\n",
            "[Step 20000] Eval: 500.0 ± 0.0\n",
            "Step   20480 | loss_pi: 0.0238  loss_v: 57.7063  entropy: 0.4863  ratio: 1.025  adv μ/σ: -0.981/13.520\n",
            "Step   22528 | loss_pi: 0.0094  loss_v: 4.7647  entropy: 0.6485  ratio: 0.986  adv μ/σ: 5.311/4.729\n",
            "Step   24576 | loss_pi: 0.0283  loss_v: 98.6775  entropy: 0.3784  ratio: 0.987  adv μ/σ: -4.809/14.907\n",
            "Step   26624 | loss_pi: 0.0361  loss_v: 1.0672  entropy: 0.6204  ratio: 0.981  adv μ/σ: 4.150/4.557\n",
            "Step   28672 | loss_pi: 0.0835  loss_v: 79.0322  entropy: 0.3580  ratio: 1.042  adv μ/σ: -9.600/17.036\n",
            "[Step 30000] Eval: 196.6 ± 15.4\n",
            "Step   30720 | loss_pi: 0.0264  loss_v: 0.5277  entropy: 0.6410  ratio: 0.937  adv μ/σ: 3.063/4.791\n",
            "Step   32768 | loss_pi: -0.0050  loss_v: 1.2479  entropy: 0.5998  ratio: 0.984  adv μ/σ: 1.769/4.017\n",
            "Step   34816 | loss_pi: 0.0595  loss_v: 22.6595  entropy: 0.4979  ratio: 0.960  adv μ/σ: -1.407/11.544\n",
            "Step   36864 | loss_pi: 0.0541  loss_v: 3.6055  entropy: 0.6240  ratio: 1.050  adv μ/σ: 2.901/4.841\n",
            "Step   38912 | loss_pi: -0.0131  loss_v: 38.2912  entropy: 0.5549  ratio: 0.986  adv μ/σ: 0.306/10.077\n",
            "[Step 40000] Eval: 353.4 ± 20.2\n",
            "Step   40960 | loss_pi: 0.0359  loss_v: 2.0628  entropy: 0.6188  ratio: 0.964  adv μ/σ: 1.511/4.821\n",
            "Step   43008 | loss_pi: 0.0446  loss_v: 61.0698  entropy: 0.4797  ratio: 0.982  adv μ/σ: -5.500/13.866\n",
            "Step   45056 | loss_pi: 0.0666  loss_v: 0.7285  entropy: 0.5942  ratio: 1.006  adv μ/σ: 2.577/3.598\n",
            "Step   47104 | loss_pi: 0.0143  loss_v: 67.0861  entropy: 0.4571  ratio: 0.926  adv μ/σ: -1.246/12.526\n",
            "Step   49152 | loss_pi: 0.0268  loss_v: 18.1185  entropy: 0.6461  ratio: 0.912  adv μ/σ: 1.916/7.559\n",
            "[Step 50000] Eval: 357.2 ± 31.0\n",
            "Step   51200 | loss_pi: 0.0694  loss_v: 48.3825  entropy: 0.5082  ratio: 0.964  adv μ/σ: -2.951/14.794\n",
            "Step   53248 | loss_pi: 0.0116  loss_v: 51.6848  entropy: 0.6310  ratio: 0.952  adv μ/σ: 1.870/10.741\n",
            "Step   55296 | loss_pi: 0.0451  loss_v: 115.0295  entropy: 0.4321  ratio: 0.988  adv μ/σ: -5.911/17.942\n",
            "Step   57344 | loss_pi: 0.0543  loss_v: 1.2042  entropy: 0.6132  ratio: 1.046  adv μ/σ: 1.076/3.959\n",
            "Step   59392 | loss_pi: 0.0272  loss_v: 33.4820  entropy: 0.4725  ratio: 1.017  adv μ/σ: -2.726/11.806\n",
            "[Step 60000] Eval: 500.0 ± 0.0\n",
            "Step   61440 | loss_pi: 0.0597  loss_v: 7.1225  entropy: 0.6271  ratio: 0.972  adv μ/σ: 1.731/5.783\n",
            "Step   63488 | loss_pi: -0.0042  loss_v: 59.7401  entropy: 0.4665  ratio: 0.934  adv μ/σ: -4.193/13.874\n",
            "Step   65536 | loss_pi: 0.1192  loss_v: 79.8720  entropy: 0.5233  ratio: 1.028  adv μ/σ: -0.192/11.665\n",
            "Step   67584 | loss_pi: 0.0499  loss_v: 2.9380  entropy: 0.5966  ratio: 0.997  adv μ/σ: -1.575/5.256\n",
            "Step   69632 | loss_pi: -0.0202  loss_v: 3.3661  entropy: 0.5948  ratio: 0.976  adv μ/σ: -2.587/5.973\n",
            "[Step 70000] Eval: 358.4 ± 62.0\n",
            "Step   71680 | loss_pi: 0.0438  loss_v: 50.6487  entropy: 0.4724  ratio: 1.014  adv μ/σ: -0.940/8.812\n",
            "Step   73728 | loss_pi: 0.0610  loss_v: 1.9414  entropy: 0.5979  ratio: 1.044  adv μ/σ: 1.807/4.045\n",
            "Step   75776 | loss_pi: -0.0210  loss_v: 5.1660  entropy: 0.6117  ratio: 0.990  adv μ/σ: 0.369/6.136\n",
            "Step   77824 | loss_pi: -0.0111  loss_v: 4.5932  entropy: 0.5309  ratio: 1.016  adv μ/σ: 1.741/4.421\n",
            "Step   79872 | loss_pi: -0.0040  loss_v: 47.8522  entropy: 0.4665  ratio: 1.050  adv μ/σ: -1.958/13.804\n",
            "[Step 80000] Eval: 500.0 ± 0.0\n",
            "Step   81920 | loss_pi: 0.0235  loss_v: 3.0014  entropy: 0.6193  ratio: 1.005  adv μ/σ: 1.658/7.041\n",
            "Step   83968 | loss_pi: -0.0263  loss_v: 41.0069  entropy: 0.4676  ratio: 0.988  adv μ/σ: -0.343/10.867\n",
            "Step   86016 | loss_pi: 0.0601  loss_v: 25.7282  entropy: 0.6083  ratio: 0.974  adv μ/σ: 0.638/10.310\n",
            "Step   88064 | loss_pi: 0.0290  loss_v: 7.7141  entropy: 0.5924  ratio: 1.043  adv μ/σ: 1.151/9.304\n",
            "[Step 90000] Eval: 500.0 ± 0.0\n",
            "Step   90112 | loss_pi: 0.0571  loss_v: 74.6508  entropy: 0.4361  ratio: 0.997  adv μ/σ: -2.083/13.938\n",
            "Step   92160 | loss_pi: 0.0037  loss_v: 30.9895  entropy: 0.5973  ratio: 1.033  adv μ/σ: 2.958/8.481\n",
            "Step   94208 | loss_pi: -0.0094  loss_v: 3.8392  entropy: 0.5715  ratio: 0.970  adv μ/σ: 1.456/6.380\n",
            "Step   96256 | loss_pi: 0.0465  loss_v: 9.8367  entropy: 0.5548  ratio: 0.958  adv μ/σ: -0.534/10.081\n",
            "Step   98304 | loss_pi: 0.0107  loss_v: 4.0397  entropy: 0.5607  ratio: 0.974  adv μ/σ: 1.901/6.947\n",
            "[Step 100000] Eval: 500.0 ± 0.0\n",
            "Step  100352 | loss_pi: 0.0249  loss_v: 10.5371  entropy: 0.5403  ratio: 1.004  adv μ/σ: 3.273/7.394\n",
            "Step  102400 | loss_pi: 0.0886  loss_v: 35.3895  entropy: 0.4143  ratio: 0.981  adv μ/σ: -4.868/13.941\n",
            "Step  104448 | loss_pi: 0.0628  loss_v: 1.2472  entropy: 0.5772  ratio: 0.978  adv μ/σ: 1.234/5.599\n",
            "Step  106496 | loss_pi: 0.0498  loss_v: 55.8193  entropy: 0.4793  ratio: 1.028  adv μ/σ: -1.922/11.148\n",
            "Step  108544 | loss_pi: 0.0303  loss_v: 0.3914  entropy: 0.5355  ratio: 1.030  adv μ/σ: 0.643/2.281\n",
            "[Step 110000] Eval: 337.0 ± 137.2\n",
            "Step  110592 | loss_pi: -0.0253  loss_v: 0.5751  entropy: 0.5443  ratio: 0.941  adv μ/σ: 1.293/1.787\n",
            "Step  112640 | loss_pi: 0.0867  loss_v: 65.0677  entropy: 0.5574  ratio: 0.981  adv μ/σ: -1.053/11.099\n",
            "Step  114688 | loss_pi: 0.0350  loss_v: 4.6162  entropy: 0.5883  ratio: 1.007  adv μ/σ: 0.461/6.201\n",
            "Step  116736 | loss_pi: -0.0068  loss_v: 49.2397  entropy: 0.5094  ratio: 1.007  adv μ/σ: -1.937/13.539\n",
            "Step  118784 | loss_pi: 0.0155  loss_v: 28.8400  entropy: 0.5006  ratio: 0.963  adv μ/σ: -3.504/9.023\n",
            "[Step 120000] Eval: 434.6 ± 130.8\n",
            "Step  120832 | loss_pi: 0.0137  loss_v: 42.5237  entropy: 0.6023  ratio: 0.991  adv μ/σ: 0.608/9.319\n",
            "Step  122880 | loss_pi: 0.0089  loss_v: 3.0232  entropy: 0.6074  ratio: 0.966  adv μ/σ: 0.544/6.935\n",
            "Step  124928 | loss_pi: 0.0161  loss_v: 4.6284  entropy: 0.5458  ratio: 1.018  adv μ/σ: 1.135/6.032\n",
            "Step  126976 | loss_pi: 0.0445  loss_v: 101.3775  entropy: 0.5870  ratio: 1.014  adv μ/σ: -0.109/10.904\n",
            "Step  129024 | loss_pi: 0.0628  loss_v: 69.6005  entropy: 0.4968  ratio: 0.975  adv μ/σ: -2.773/15.190\n",
            "[Step 130000] Eval: 253.6 ± 14.4\n",
            "Step  131072 | loss_pi: -0.0109  loss_v: 11.3097  entropy: 0.5418  ratio: 1.002  adv μ/σ: 1.842/5.390\n",
            "Step  133120 | loss_pi: -0.0176  loss_v: 41.4630  entropy: 0.5395  ratio: 0.949  adv μ/σ: 0.350/10.641\n",
            "Step  135168 | loss_pi: -0.0129  loss_v: 63.0936  entropy: 0.6077  ratio: 1.027  adv μ/σ: 0.307/11.454\n",
            "Step  137216 | loss_pi: 0.0059  loss_v: 27.2246  entropy: 0.5382  ratio: 0.990  adv μ/σ: -1.291/10.596\n",
            "Step  139264 | loss_pi: 0.0033  loss_v: 2.5639  entropy: 0.5343  ratio: 1.019  adv μ/σ: 1.563/6.446\n",
            "[Step 140000] Eval: 500.0 ± 0.0\n",
            "Step  141312 | loss_pi: -0.0071  loss_v: 14.1377  entropy: 0.5983  ratio: 0.992  adv μ/σ: 1.752/5.826\n",
            "Step  143360 | loss_pi: 0.0092  loss_v: 49.9547  entropy: 0.5583  ratio: 1.013  adv μ/σ: -0.666/12.229\n",
            "Step  145408 | loss_pi: 0.0392  loss_v: 19.5040  entropy: 0.5517  ratio: 1.026  adv μ/σ: -0.840/12.996\n",
            "Step  147456 | loss_pi: 0.0772  loss_v: 7.6153  entropy: 0.5884  ratio: 0.986  adv μ/σ: 0.656/8.543\n"
          ]
        }
      ],
      "source": [
        "# PPO (TensorFlow 2.x) for CartPole-v1\n",
        "# - GAE(Generalized Advantage Estimation)\n",
        "# - clipped surrogate objective (epsilon=0.2)\n",
        "# - value loss + entropy bonus\n",
        "# - multi-epoch updates with minibatches\n",
        "# - gymnasium 또는 gym 자동 호환\n",
        "\n",
        "import os, sys, math, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ===== Gym / Gymnasium 호환 =====\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    GYMN = True\n",
        "except Exception:\n",
        "    import gym\n",
        "    GYMN = False\n",
        "\n",
        "print(\"Using:\", \"gymnasium\" if GYMN else \"gym\")\n",
        "ENV_ID = \"CartPole-v1\"\n",
        "\n",
        "# ===== 하이퍼파라미터 =====\n",
        "seed               = 42\n",
        "total_timesteps    = 200_000\n",
        "steps_per_rollout  = 2048\n",
        "gamma              = 0.99\n",
        "gae_lambda         = 0.95\n",
        "learning_rate      = 3e-4\n",
        "clip_eps           = 0.2\n",
        "value_coef         = 0.5\n",
        "entropy_coef       = 0.01\n",
        "update_epochs      = 10\n",
        "minibatch_size     = 64\n",
        "max_grad_norm      = 0.5\n",
        "eval_interval      = 10_000   # 평가 주기(스텝)\n",
        "render_eval        = False\n",
        "\n",
        "# ===== 시드 고정 =====\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# ===== 환경 생성 =====\n",
        "if GYMN:\n",
        "    env = gym.make(ENV_ID)\n",
        "    eval_env = gym.make(ENV_ID)\n",
        "    obs, info = env.reset(seed=seed)\n",
        "else:\n",
        "    env = gym.make(ENV_ID)\n",
        "    eval_env = gym.make(ENV_ID)\n",
        "    env.seed(seed)\n",
        "    eval_env.seed(seed)\n",
        "    obs = env.reset()\n",
        "\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# ===== 네트워크 정의 (공유 본체 + 정책/가치 헤드) =====\n",
        "class ActorCritic(tf.keras.Model):\n",
        "    def __init__(self, n_actions):\n",
        "        super().__init__()\n",
        "        # 공유 본체\n",
        "        self.fc1 = tf.keras.layers.Dense(128, activation='tanh')\n",
        "        self.fc2 = tf.keras.layers.Dense(128, activation='tanh')\n",
        "        # 정책/가치 헤드\n",
        "        self.logits = tf.keras.layers.Dense(n_actions, activation=None)\n",
        "        self.value  = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "        h = self.fc1(x)\n",
        "        h = self.fc2(h)\n",
        "        return self.logits(h), tf.squeeze(self.value(h), axis=-1)\n",
        "\n",
        "policy = ActorCritic(n_actions)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# ===== Rollout 버퍼 =====\n",
        "class RolloutBuffer:\n",
        "    def __init__(self, size, obs_dim):\n",
        "        self.size = size\n",
        "        self.obs = np.zeros((size, obs_dim), dtype=np.float32)\n",
        "        self.actions = np.zeros((size,), dtype=np.int32)\n",
        "        self.logprobs = np.zeros((size,), dtype=np.float32)\n",
        "        self.rewards = np.zeros((size,), dtype=np.float32)\n",
        "        self.dones = np.zeros((size,), dtype=np.float32)\n",
        "        self.values = np.zeros((size,), dtype=np.float32)\n",
        "        # post-compute\n",
        "        self.advantages = np.zeros((size,), dtype=np.float32)\n",
        "        self.returns = np.zeros((size,), dtype=np.float32)\n",
        "        self.ptr = 0\n",
        "\n",
        "    def add(self, o, a, lp, r, d, v):\n",
        "        self.obs[self.ptr] = o\n",
        "        self.actions[self.ptr] = a\n",
        "        self.logprobs[self.ptr] = lp\n",
        "        self.rewards[self.ptr] = r\n",
        "        self.dones[self.ptr] = d\n",
        "        self.values[self.ptr] = v\n",
        "        self.ptr += 1\n",
        "\n",
        "    def full(self):\n",
        "        return self.ptr >= self.size\n",
        "\n",
        "    def reset(self):\n",
        "        self.ptr = 0\n",
        "\n",
        "buffer = RolloutBuffer(steps_per_rollout, obs_dim)\n",
        "\n",
        "# ===== 정책으로 행동 샘플링/로그확률/가치 =====\n",
        "@tf.function\n",
        "def policy_step(obs_batch):\n",
        "    logits, value = policy(obs_batch)\n",
        "    action_dist = tf.random.categorical(logits, num_samples=1)  # [B,1]\n",
        "    action = tf.squeeze(action_dist, axis=1)\n",
        "    # 로그확률 계산 (categorical)\n",
        "    logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels=action, logits=logits)\n",
        "    return action, logprob, value\n",
        "\n",
        "def get_action_and_logprob_value(obs):\n",
        "    # 단일 관측치 버전\n",
        "    logits, value = policy(obs[None, :])\n",
        "    action = tf.random.categorical(logits, 1)\n",
        "    action = int(action.numpy().squeeze())\n",
        "    # 로그확률\n",
        "    logprob = -tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=[action], logits=logits\n",
        "    ).numpy().squeeze()\n",
        "    return action, float(logprob), float(value.numpy().squeeze())\n",
        "\n",
        "# ===== GAE 계산 =====\n",
        "def compute_gae(rewards, dones, values, last_value, gamma=0.99, lam=0.95):\n",
        "    T = len(rewards)\n",
        "    advantages = np.zeros_like(rewards, dtype=np.float32)\n",
        "    gae = 0.0\n",
        "    for t in reversed(range(T)):\n",
        "        next_value = last_value if t == T-1 else values[t+1]\n",
        "        delta = rewards[t] + gamma * (1.0 - dones[t]) * next_value - values[t]\n",
        "        gae = delta + gamma * lam * (1.0 - dones[t]) * gae\n",
        "        advantages[t] = gae\n",
        "    returns = advantages + values\n",
        "    return advantages, returns\n",
        "\n",
        "# ===== PPO 업데이트 스텝 =====\n",
        "@tf.function\n",
        "def ppo_update(obs, actions, old_logprobs, returns, advantages):\n",
        "    # 표준화된 advantage\n",
        "    adv_mean = tf.reduce_mean(advantages)\n",
        "    adv_std = tf.math.reduce_std(advantages) + 1e-8\n",
        "    norm_adv = (advantages - adv_mean) / adv_std\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits, values = policy(obs)\n",
        "        # 새 로그확률\n",
        "        new_logprobs = -tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=tf.cast(actions, tf.int32), logits=logits\n",
        "        )\n",
        "        # ratio = exp(new - old)\n",
        "        ratio = tf.exp(new_logprobs - old_logprobs)\n",
        "\n",
        "        # policy loss (clipped)\n",
        "        unclipped = ratio * norm_adv\n",
        "        clipped = tf.clip_by_value(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * norm_adv\n",
        "        policy_loss = -tf.reduce_mean(tf.minimum(unclipped, clipped))\n",
        "\n",
        "        # value loss (MSE)\n",
        "        value_loss = tf.reduce_mean(tf.square(returns - values)) * value_coef\n",
        "\n",
        "        # entropy bonus\n",
        "        # -sum p*logp = categorical entropy\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        entropy = -tf.reduce_mean(tf.reduce_sum(probs * tf.math.log(tf.clip_by_value(probs, 1e-8, 1.0)), axis=1))\n",
        "        entropy_loss = -entropy_coef * entropy  # (minus because we minimize total loss)\n",
        "\n",
        "        total_loss = policy_loss + value_loss + entropy_loss\n",
        "\n",
        "    grads = tape.gradient(total_loss, policy.trainable_variables)\n",
        "    # gradient clipping\n",
        "    grads, _ = tf.clip_by_global_norm(grads, max_grad_norm)\n",
        "    optimizer.apply_gradients(zip(grads, policy.trainable_variables))\n",
        "    return policy_loss, value_loss, entropy, tf.reduce_mean(ratio)\n",
        "\n",
        "# ===== 평가 함수 =====\n",
        "def evaluate(env, policy, episodes=5, render=False):\n",
        "    scores = []\n",
        "    for _ in range(episodes):\n",
        "        if GYMN:\n",
        "            obs, info = env.reset(seed=np.random.randint(1e9))\n",
        "        else:\n",
        "            obs = env.reset()\n",
        "        done = False\n",
        "        ep_ret = 0\n",
        "        while not done:\n",
        "            logits, _ = policy(np.array(obs)[None, :])\n",
        "            action = int(tf.argmax(logits, axis=-1).numpy().squeeze())\n",
        "            if GYMN:\n",
        "                next_obs, reward, term, trunc, info = env.step(action)\n",
        "                done = bool(term or trunc)\n",
        "            else:\n",
        "                next_obs, reward, done, info = env.step(action)\n",
        "            ep_ret += reward\n",
        "            obs = next_obs\n",
        "            if render:\n",
        "                env.render()\n",
        "        scores.append(ep_ret)\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "# ===== 학습 루프 =====\n",
        "global_step = 0\n",
        "best_score = -np.inf\n",
        "\n",
        "if GYMN:\n",
        "    obs, info = env.reset(seed=seed)\n",
        "else:\n",
        "    obs = env.reset()\n",
        "\n",
        "episode_return = 0\n",
        "episode_len = 0\n",
        "\n",
        "print(\"Start training PPO on\", ENV_ID)\n",
        "while global_step < total_timesteps:\n",
        "    buffer.reset()\n",
        "    # ---- Rollout 수집 ----\n",
        "    for t in range(steps_per_rollout):\n",
        "        action, logprob, value = get_action_and_logprob_value(np.array(obs, dtype=np.float32))\n",
        "        if GYMN:\n",
        "            next_obs, reward, term, trunc, info = env.step(action)\n",
        "            done = bool(term or trunc)\n",
        "        else:\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.add(np.array(obs, dtype=np.float32), action, logprob, reward, float(done), value)\n",
        "\n",
        "        obs = next_obs\n",
        "        episode_return += reward\n",
        "        episode_len += 1\n",
        "        global_step += 1\n",
        "\n",
        "        if done:\n",
        "            if GYMN:\n",
        "                obs, info = env.reset()\n",
        "            else:\n",
        "                obs = env.reset()\n",
        "            episode_return = 0\n",
        "            episode_len = 0\n",
        "\n",
        "        if global_step % eval_interval == 0:\n",
        "            mean_score, std_score = evaluate(eval_env, policy, episodes=5, render=render_eval)\n",
        "            print(f\"[Step {global_step}] Eval: {mean_score:.1f} ± {std_score:.1f}\")\n",
        "\n",
        "    # ---- 부트스트랩 값 ----\n",
        "    with tf.device(\"/CPU:0\"):\n",
        "        # 마지막 next value\n",
        "        logits, last_v = policy(np.array(obs, dtype=np.float32)[None, :])\n",
        "        last_value = float(last_v.numpy().squeeze())\n",
        "\n",
        "    # ---- GAE/리턴 계산 ----\n",
        "    adv, ret = compute_gae(buffer.rewards, buffer.dones, buffer.values, last_value,\n",
        "                           gamma=gamma, lam=gae_lambda)\n",
        "    buffer.advantages = adv\n",
        "    buffer.returns = ret\n",
        "\n",
        "    # ---- 미니배치 학습 ----\n",
        "    idxs = np.arange(steps_per_rollout)\n",
        "    for epoch in range(update_epochs):\n",
        "        np.random.shuffle(idxs)\n",
        "        for start in range(0, steps_per_rollout, minibatch_size):\n",
        "            end = start + minibatch_size\n",
        "            mb_idx = idxs[start:end]\n",
        "\n",
        "            mb_obs = tf.convert_to_tensor(buffer.obs[mb_idx], dtype=tf.float32)\n",
        "            mb_actions = tf.convert_to_tensor(buffer.actions[mb_idx], dtype=tf.int32)\n",
        "            mb_old_logprobs = tf.convert_to_tensor(buffer.logprobs[mb_idx], dtype=tf.float32)\n",
        "            mb_returns = tf.convert_to_tensor(buffer.returns[mb_idx], dtype=tf.float32)\n",
        "            mb_advs = tf.convert_to_tensor(buffer.advantages[mb_idx], dtype=tf.float32)\n",
        "\n",
        "            pl, vl, ent, rat = ppo_update(mb_obs, mb_actions, mb_old_logprobs, mb_returns, mb_advs)\n",
        "\n",
        "    # ---- 간단한 훈련 로그 ----\n",
        "    mean_adv = float(np.mean(buffer.advantages))\n",
        "    std_adv = float(np.std(buffer.advantages) + 1e-8)\n",
        "    print(f\"Step {global_step:7d} | loss_pi: {pl.numpy():.4f}  loss_v: {vl.numpy():.4f}  \"\n",
        "          f\"entropy: {ent.numpy():.4f}  ratio: {rat.numpy():.3f}  adv μ/σ: {mean_adv:.3f}/{std_adv:.3f}\")\n",
        "\n",
        "# 최종 평가\n",
        "final_mean, final_std = evaluate(eval_env, policy, episodes=10, render=False)\n",
        "print(f\"Training finished. Final Eval: {final_mean:.1f} ± {final_std:.1f}\")\n",
        "\n",
        "# 모델 저장 (원하면 불러오기: tf.keras.models.save_model / tf.keras.models.load_model)\n",
        "save_path = \"ppo_cartpole_actor_critic\"\n",
        "policy.save_weights(save_path)\n",
        "print(\"Saved weights to:\", save_path)"
      ]
    }
  ]
}