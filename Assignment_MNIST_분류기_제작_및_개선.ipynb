{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2KESfw0wVaoEEblTiUIS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/DeepLearning/blob/main/Assignment_MNIST_%EB%B6%84%EB%A5%98%EA%B8%B0_%EC%A0%9C%EC%9E%91_%EB%B0%8F_%EA%B0%9C%EC%84%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR3MDm4pmlRf"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# 📘 MNIST 분류기 종합 프로젝트 (과제용 템플릿)\n",
        "# =============================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.initializers import GlorotUniform, HeNormal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1. 데이터셋 로드 & 전처리\n",
        "# -----------------------------\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# TODO 1: 데이터 정규화 (0~1 범위로 맞추기)\n",
        "x_train = ...\n",
        "x_test = ...\n",
        "\n",
        "num_classes = 10\n",
        "# TODO 2: One-hot encoding 적용\n",
        "y_train = ...\n",
        "y_test = ...\n",
        "\n",
        "# -----------------------------\n",
        "# 2. 모델 빌더 함수\n",
        "# -----------------------------\n",
        "def build_mlp(use_dropout=False, use_batchnorm=False,\n",
        "              initializer=\"glorot_uniform\", optimizer=\"adam\"):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28,28)))\n",
        "\n",
        "    # TODO 3: 첫 번째 은닉층 (256 뉴런, relu 활성화, initializer 사용)\n",
        "    model.add(Dense(..., activation=\"relu\", kernel_initializer=initializer))\n",
        "\n",
        "    # TODO 4: BatchNorm / Dropout 적용 여부\n",
        "    if use_batchnorm:\n",
        "        model.add(...)\n",
        "    if use_dropout:\n",
        "        model.add(...)\n",
        "\n",
        "    # TODO 5: 두 번째 은닉층 (128 뉴런, relu 활성화, initializer 사용)\n",
        "    model.add(Dense(..., activation=\"relu\", kernel_initializer=initializer))\n",
        "    if use_batchnorm:\n",
        "        model.add(...)\n",
        "    if use_dropout:\n",
        "        model.add(...)\n",
        "\n",
        "    # 출력층\n",
        "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "    # TODO 6: 옵티마이저 선택\n",
        "    if optimizer == \"sgd\":\n",
        "        opt = ...\n",
        "    elif optimizer == \"rmsprop\":\n",
        "        opt = ...\n",
        "    else:  # 기본 adam\n",
        "        opt = ...\n",
        "\n",
        "    # 모델 컴파일\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# 3. 실험 시나리오 정의\n",
        "# -----------------------------\n",
        "experiments = [\n",
        "    {\"name\": \"Baseline-MLP\", \"dropout\": False, \"batchnorm\": False, \"init\": \"glorot_uniform\", \"opt\": \"adam\"},\n",
        "    {\"name\": \"Dropout\",      \"dropout\": True,  \"batchnorm\": False, \"init\": \"glorot_uniform\", \"opt\": \"adam\"},\n",
        "    {\"name\": \"BatchNorm\",    \"dropout\": False, \"batchnorm\": True,  \"init\": \"glorot_uniform\", \"opt\": \"adam\"},\n",
        "    {\"name\": \"Dropout+BN\",   \"dropout\": True,  \"batchnorm\": True,  \"init\": \"glorot_uniform\", \"opt\": \"adam\"},\n",
        "    {\"name\": \"HeInit+Adam\",  \"dropout\": False, \"batchnorm\": True,  \"init\": HeNormal(),       \"opt\": \"adam\"},\n",
        "    {\"name\": \"BN+SGD\",       \"dropout\": False, \"batchnorm\": True,  \"init\": \"glorot_uniform\", \"opt\": \"sgd\"},\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# 4. 실험 실행\n",
        "# -----------------------------\n",
        "results = {}\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "for exp in experiments:\n",
        "    print(f\"\\n🔹 Running Experiment: {exp['name']}\")\n",
        "\n",
        "    # TODO 7: 모델 생성\n",
        "    model = build_mlp(use_dropout=exp[\"dropout\"],\n",
        "                      use_batchnorm=exp[\"batchnorm\"],\n",
        "                      initializer=exp[\"init\"],\n",
        "                      optimizer=exp[\"opt\"])\n",
        "\n",
        "    # TODO 8: 학습 (validation_split=0.2 사용)\n",
        "    history = model.fit(...)\n",
        "\n",
        "    # TODO 9: 테스트 정확도 평가\n",
        "    test_loss, test_acc = model.evaluate(..., ..., verbose=0)\n",
        "    results[exp[\"name\"]] = {\"acc\": test_acc, \"loss\": test_loss, \"history\": history}\n",
        "\n",
        "# -----------------------------\n",
        "# 5. 결과 비교 시각화\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(12,6))\n",
        "for name, res in results.items():\n",
        "    plt.plot(res[\"history\"].history[\"val_accuracy\"], label=f\"{name} (Acc={res['acc']:.3f})\")\n",
        "plt.title(\"Validation Accuracy Comparison\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 6. 최고 성능 모델 찾기\n",
        "# -----------------------------\n",
        "best_model = max(results.items(), key=lambda x: x[1][\"acc\"])\n",
        "print(\"\\n✅ Best Model:\", best_model[0])\n",
        "print(f\"   Test Accuracy: {best_model[1]['acc']:.4f}, Test Loss: {best_model[1]['loss']:.4f}\")\n"
      ]
    }
  ]
}