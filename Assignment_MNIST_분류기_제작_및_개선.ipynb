{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2KESfw0wVaoEEblTiUIS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/DeepLearning/blob/main/Assignment_MNIST_%EB%B6%84%EB%A5%98%EA%B8%B0_%EC%A0%9C%EC%9E%91_%EB%B0%8F_%EA%B0%9C%EC%84%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR3MDm4pmlRf"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# ğŸ“˜ MNIST ë¶„ë¥˜ê¸° ì¢…í•© í”„ë¡œì íŠ¸ (ê³¼ì œìš© í…œí”Œë¦¿)\n",
        "# =============================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.initializers import GlorotUniform, HeNormal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1. ë°ì´í„°ì…‹ ë¡œë“œ & ì „ì²˜ë¦¬\n",
        "# -----------------------------\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# TODO 1: ë°ì´í„° ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ ë§ì¶”ê¸°)\n",
        "x_train = ...\n",
        "x_test = ...\n",
        "\n",
        "num_classes = 10\n",
        "# TODO 2: One-hot encoding ì ìš©\n",
        "y_train = ...\n",
        "y_test = ...\n",
        "\n",
        "# -----------------------------\n",
        "# 2. ëª¨ë¸ ë¹Œë” í•¨ìˆ˜\n",
        "# -----------------------------\n",
        "def build_mlp(use_dropout=False, use_batchnorm=False,\n",
        "              initializer=\"glorot_uniform\", optimizer=\"adam\"):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28,28)))\n",
        "\n",
        "    # TODO 3: ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ (256 ë‰´ëŸ°, relu í™œì„±í™”, initializer ì‚¬ìš©)\n",
        "    model.add(Dense(..., activation=\"relu\", kernel_initializer=initializer))\n",
        "\n",
        "    # TODO 4: BatchNorm / Dropout ì ìš© ì—¬ë¶€\n",
        "    if use_batchnorm:\n",
        "        model.add(...)\n",
        "    if use_dropout:\n",
        "        model.add(...)\n",
        "\n",
        "    # TODO 5: ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ (128 ë‰´ëŸ°, relu í™œì„±í™”, initializer ì‚¬ìš©)\n",
        "    model.add(Dense(..., activation=\"relu\", kernel_initializer=initializer))\n",
        "    if use_batchnorm:\n",
        "        model.add(...)\n",
        "    if use_dropout:\n",
        "        model.add(...)\n",
        "\n",
        "    # ì¶œë ¥ì¸µ\n",
        "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "    # TODO 6: ì˜µí‹°ë§ˆì´ì € ì„ íƒ\n",
        "    if optimizer == \"sgd\":\n",
        "        opt = ...\n",
        "    elif optimizer == \"rmsprop\":\n",
        "        opt = ...\n",
        "    else:  # ê¸°ë³¸ adam\n",
        "        opt = ...\n",
        "\n",
        "    # ëª¨ë¸ ì»´íŒŒì¼\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# 3. ì‹¤í—˜ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜\n",
        "# -----------------------------\n",
        "experiments = [\n",
        "    {\"name\": \"Baseline-MLP\", \"dropout\": False, \"batchnorm\": False, \"init\": \"glorot_uniform\", \"opt\": \"adam\"},\n",
        "    {\"name\": \"Dropout\",      \"dropout\": True,  \"batchnorm\": False, \"init\": \"glorot_uniform\", \"opt\": \"adam\"},\n",
        "    {\"name\": \"BatchNorm\",    \"dropout\": False, \"batchnorm\": True,  \"init\": \"glorot_uniform\", \"opt\": \"adam\"},\n",
        "    {\"name\": \"Dropout+BN\",   \"dropout\": True,  \"batchnorm\": True,  \"init\": \"glorot_uniform\", \"opt\": \"adam\"},\n",
        "    {\"name\": \"HeInit+Adam\",  \"dropout\": False, \"batchnorm\": True,  \"init\": HeNormal(),       \"opt\": \"adam\"},\n",
        "    {\"name\": \"BN+SGD\",       \"dropout\": False, \"batchnorm\": True,  \"init\": \"glorot_uniform\", \"opt\": \"sgd\"},\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# 4. ì‹¤í—˜ ì‹¤í–‰\n",
        "# -----------------------------\n",
        "results = {}\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "for exp in experiments:\n",
        "    print(f\"\\nğŸ”¹ Running Experiment: {exp['name']}\")\n",
        "\n",
        "    # TODO 7: ëª¨ë¸ ìƒì„±\n",
        "    model = build_mlp(use_dropout=exp[\"dropout\"],\n",
        "                      use_batchnorm=exp[\"batchnorm\"],\n",
        "                      initializer=exp[\"init\"],\n",
        "                      optimizer=exp[\"opt\"])\n",
        "\n",
        "    # TODO 8: í•™ìŠµ (validation_split=0.2 ì‚¬ìš©)\n",
        "    history = model.fit(...)\n",
        "\n",
        "    # TODO 9: í…ŒìŠ¤íŠ¸ ì •í™•ë„ í‰ê°€\n",
        "    test_loss, test_acc = model.evaluate(..., ..., verbose=0)\n",
        "    results[exp[\"name\"]] = {\"acc\": test_acc, \"loss\": test_loss, \"history\": history}\n",
        "\n",
        "# -----------------------------\n",
        "# 5. ê²°ê³¼ ë¹„êµ ì‹œê°í™”\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(12,6))\n",
        "for name, res in results.items():\n",
        "    plt.plot(res[\"history\"].history[\"val_accuracy\"], label=f\"{name} (Acc={res['acc']:.3f})\")\n",
        "plt.title(\"Validation Accuracy Comparison\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 6. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°\n",
        "# -----------------------------\n",
        "best_model = max(results.items(), key=lambda x: x[1][\"acc\"])\n",
        "print(\"\\nâœ… Best Model:\", best_model[0])\n",
        "print(f\"   Test Accuracy: {best_model[1]['acc']:.4f}, Test Loss: {best_model[1]['loss']:.4f}\")\n"
      ]
    }
  ]
}